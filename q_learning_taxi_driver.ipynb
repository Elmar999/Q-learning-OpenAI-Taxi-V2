{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|R: | : :G|\n| : | : :\u001b[43m \u001b[0m|\n| : : : : |\n| | : | : |\n|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6, 500)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#Sample actions for exploration:\n",
    "env.action_space.n, env.observation_space.n\n",
    "# print(env.step(1))\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "State: 454\n+---------+\n|R: | : :G|\n| : | : :\u001b[43m \u001b[0m|\n| : : : : |\n| | : | : |\n|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(4, 2, 3, 2) \n",
    "print(\"State:\", state)\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "num_episodes = 15000 #20000 #60000\n",
    "gamma = 0.95 #0.99\n",
    "learning_rate = 0.1 #0.95 #0.85\n",
    "epsilon = 0.3#1 #0.15 #0.1\n",
    "\n",
    "# initialize the Q table\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_Qtable(Q, env, num_episodes, epsilon, gamma, lr_rate):\n",
    "    '''\n",
    "    function trains Q table with given parameters\n",
    "    Args:\n",
    "        Q (numpy array): Q table which will be updated\n",
    "        env (gym environment)\n",
    "        num_episodes (int): number of games that will be played during training\n",
    "        epsilon (int): probability threshold\n",
    "        gamma (int): discount rate\n",
    "        lr_rate (int): learning rate\n",
    "    Returns:\n",
    "        Q_optimal (numpy array): updated Q table which is converged to optimal\n",
    "    '''\n",
    "\n",
    "    Q_old = Q.copy()\n",
    "    for i in range(num_episodes):\n",
    "        # define initial state\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while done == False:\n",
    "            # First we select an action:\n",
    "            if random.uniform(0, 1) < epsilon: # take a random number\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(Q[state,:]) # Exploit learned values\n",
    "            # Then we perform the action and receive the feedback from the environment\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            # Finally we learn from the experience by updating the Q-value of the selected action\n",
    "            update = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
    "            Q[state,action] += learning_rate*update \n",
    "            if (Q_old == Q).all():\n",
    "                print(\"Q table has been converged to optimal in {}th iteration \".format(i))\n",
    "                return Q\n",
    "            Q_old = Q.copy()\n",
    "            state = new_state\n",
    "\n",
    "    # even if Q table will not converge to optimal return latest updated Q table\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q table has been converged to optimal in 1481th iteration \n"
     ]
    }
   ],
   "source": [
    "# train Q table\n",
    "Q_optimal = train_Qtable(Q, env, num_episodes, epsilon, gamma, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q table with optimal values:\n [[ 0.          0.          0.          0.          0.          0.        ]\n [-1.66748262 -1.60343154 -3.42653199 -0.24645887  5.07306024 -8.50226698]\n [ 3.1020709   2.90835883 -0.17783154  1.67353439 10.93790206 -2.6644929 ]\n ...\n [-0.63943171  5.98714514 -1.16298197 -0.10183063 -5.05831399 -3.97650524]\n [-2.56659323 -2.54138504 -2.30179111  1.18526024 -5.2850076  -9.26522599]\n [-0.271       1.00335038  2.65660139 16.25273594  0.143404   -0.15508807]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q table with optimal values:\\n\", Q_optimal )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_game(Q, env):\n",
    "    '''\n",
    "    launch game with optimal Q value\n",
    "    Args:\n",
    "        Q (numpy array): Q table with optimal values\n",
    "        env (gym environment)\n",
    "    '''\n",
    "\n",
    "    # define initial state\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    while done == False:\n",
    "        # Take the action (index) with the maximum expected discounted future reward given that state\n",
    "        action = np.argmax(Q[state,:])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nlaunch game with optimal Q values\n\n+---------+\n|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n|\u001b[43m \u001b[0m: | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n\n+---------+\n|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (North)\n+---------+\n|\u001b[42mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (Pickup)\n+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n|\u001b[42m_\u001b[0m: | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (South)\n+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n|\u001b[42m_\u001b[0m: : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (South)\n+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| :\u001b[42m_\u001b[0m: : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (East)\n+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| : :\u001b[42m_\u001b[0m: : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (East)\n+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| : : :\u001b[42m_\u001b[0m: |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (East)\n+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| : : : :\u001b[42m_\u001b[0m|\n| | : | : |\n|Y| : |B: |\n+---------+\n  (East)\n+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : :\u001b[42m_\u001b[0m|\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (North)\n+---------+\n|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (North)\n+---------+\n|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nlaunch game with optimal Q values\\n\")\n",
    "launch_game(Q_optimal, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}